{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Structure Machine Translation\n",
    "\n",
    "Based on the succes of Neural Machine Translation, I'm going to try a seq2seq type of model. The encoder with be the same as the current RGN model; instead of predicting torsion angles directly after the encoder, I will implement a decoder network with attention to assist in the folding process. First and foremost I hope that the shorter distance between the trainable parameters and the evaluation of loss will improve the flow of gradients (this argument doesn't make much sense though). More importantly, the network allows information from the folding process to be directly incorporated back into the trainable parameters. There is also the possibility that by using teacher forcing (feeding in the real output to the next decoder timestep) the network will be able to learn about the folding process in addition to the features of the input sequence.\n",
    "\n",
    "The main inspiration for this approach comes from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as ip\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "#import utils\n",
    "from fastai import *\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from collections import Counter as cs\n",
    "#import nglview as nv\n",
    "import sys\n",
    "import Bio.PDB as bio\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import torch.optim\n",
    "import pdb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from data import ProteinDataset, sequence_collate\n",
    "from model import geometric_unit, dRMSD\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.curdir + '/data/'\n",
    "pdb_path = os.curdir + '/data/pdb/structures/pdb/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProteinDataset(data_path, 'short', encoding='tokens')\n",
    "trn_data = DataLoader(dataset, batch_size=14, shuffle=True, collate_fn=sequence_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa2vec = bcolz.open(data_path + 'c3_embs.bc')\n",
    "\n",
    "def create_emb_layer(aa2vec, requires_grad=True):\n",
    "    aa2vec = torch.tensor(aa2vec, requires_grad=requires_grad)\n",
    "    #blank_vec = torch.zeros(aa2vec.shape[1]).view(1,-1)\n",
    "    #aa2vec = torch.cat([aa2vec, blank_vec], dim=0)\n",
    "    \n",
    "    vocab_sz, embed_dim = aa2vec.size()\n",
    "    emb_layer = nn.Embedding(vocab_sz, embed_dim)\n",
    "    emb_layer.load_state_dict({'weight': aa2vec})\n",
    "    if requires_grad == False:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, vocab_sz, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinEncoder(nn.Module):\n",
    "    def __init__(self, aa2vec, hidden_size, num_layers):\n",
    "        super(ProteinEncoder, self).__init__()\n",
    "        \n",
    "        input_size = aa2vec.shape[1]\n",
    "        self.aa2vec = aa2vec\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embeds, vocab_sz, embed_dim = create_emb_layer(aa2vec, requires_grad=True)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp_seq = x[0]\n",
    "        inp_lens = x[1]\n",
    "        order = [x for x,y in sorted(enumerate(inp_lens), key=lambda x: x[1], reverse=True)]\n",
    "        \n",
    "        #forward propagate lstm\n",
    "        emb = self.embeds(inp_seq)\n",
    "        packed = pack_padded_sequence(emb[:, order], inp_lens[order], batch_first=False)\n",
    "        \n",
    "        gru_out, hid_out = self.gru(packed, None)\n",
    "        unpacked, _ = pad_packed_sequence(gru_out, batch_first=False, padding_value=0.0)\n",
    "        output = unpacked[:, range(inp_seq.size(1))] #reorder to match target\n",
    "\n",
    "        #sum the context vectors\n",
    "        outputs = unpacked[:, :, :self.hidden_size] + unpacked[:, :, self.hidden_size:]\n",
    "        \n",
    "        return outputs, hid_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, torch.Size([29, 14]), torch.Size([87, 14, 3]), torch.Size([29, 14, 20]), torch.Size([2, 14, 20]))\n",
      "(1, torch.Size([29, 14]), torch.Size([87, 14, 3]), torch.Size([29, 14, 20]), torch.Size([2, 14, 20]))\n",
      "(2, torch.Size([29, 14]), torch.Size([87, 14, 3]), torch.Size([29, 14, 20]), torch.Size([2, 14, 20]))\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sampled_batch in enumerate(trn_data):\n",
    "    inp_seq = torch.tensor(sampled_batch['sequence'], dtype=torch.long, requires_grad=True)\n",
    "    inp_lens = torch.tensor(sampled_batch['length'], dtype=torch.long, requires_grad=False)\n",
    "    enc = ProteinEncoder(aa2vec,20,1)\n",
    "    enc_out, enc_hi = enc([inp_seq, inp_lens])\n",
    "    print(i_batch, inp_seq.size(), sampled_batch['coords'].size(), enc_out.size(), enc_hi.size())\n",
    "    \n",
    "    if i_batch == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hid_sz):\n",
    "        super(Attn, self).__init__()\n",
    "        #self.method = method\n",
    "        self.hid_sz = hid_sz\n",
    "        \n",
    "        #if self.method == 'general':\n",
    "        self.attn = nn.Linear(hid_sz, hid_sz)\n",
    "        #elif self.method == 'concat':\n",
    "        #    self.attn = nn.Linear(hidden_size*2, hidden_size)\n",
    "        #    self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "            \n",
    "    def forward(self, prev_hi, enc_out):\n",
    "        max_len = enc_out.size(0)\n",
    "        bs = enc_out.size(1)\n",
    "        \n",
    "        attn_energies = torch.zeros((bs, max_len))\n",
    "        \n",
    "        for b in range(bs):\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(prev_hi[0, b], enc_out[i, b])\n",
    "        \n",
    "        return F.softmax(attn_energies, dim=-1)\n",
    "    \n",
    "    def score(self, hidden, enc_out):\n",
    "        \n",
    "        #if self.method == 'dot':\n",
    "        #    energy = hidden.mm(enc_out)\n",
    "        #    return energy\n",
    "        \n",
    "        #elif self.method == 'general':\n",
    "        energy = self.attn(enc_out)\n",
    "        energy = hidden.dot(energy)\n",
    "        return energy\n",
    "        \n",
    "        #TODO: make this work?\n",
    "        #elif self.method == 'concat':\n",
    "        #    energy = self.attn(torch.cat((hidden, enc_out), 1))\n",
    "        #    energy = self.v.mm(energy)\n",
    "        #    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 29])\n",
      "torch.Size([14, 29])\n",
      "torch.Size([14, 29])\n"
     ]
    }
   ],
   "source": [
    "hid_sz = 20\n",
    "for i_batch, sampled_batch in enumerate(trn_data):\n",
    "    inp_seq = torch.tensor(sampled_batch['sequence'], dtype=torch.long, requires_grad=True)\n",
    "    inp_lens = torch.tensor(sampled_batch['length'], dtype=torch.long, requires_grad=False)\n",
    "    enc = ProteinEncoder(aa2vec,hid_sz,1)\n",
    "    enc_out, enc_hi = enc([inp_seq, inp_lens])\n",
    "    dec_hi = enc_hi[:1]\n",
    "    \n",
    "    attn = Attn(hid_sz)\n",
    "    attn_out = attn(dec_hi, enc_out)\n",
    "    print(attn_out.size())\n",
    "    #softmax in correct dimension because each sample in batch adds to 1\n",
    "    \n",
    "    if i_batch == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDecoder(nn.Module):\n",
    "    def __init__(self, aa2vec, hid_sz, num_layers):\n",
    "        super(ProteinDecoder, self).__init__()\n",
    "        \n",
    "        input_size = aa2vec.shape[1]\n",
    "        self.aa2vec = aa2vec\n",
    "        self.hid_sz = hid_sz\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embeds, vocab_sz, embed_dim = create_emb_layer(aa2vec, requires_grad=True)\n",
    "        #self.attn = Attn(hid_sz)\n",
    "        self.gru = nn.GRU(hid_sz+embed_dim, hid_sz, num_layers)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size, 3)\n",
    "        self.linear2 = nn.Linear(hidden_size, 3)\n",
    "        self.hardtanh = nn.Hardtanh()\n",
    "        \n",
    "        #as per Mohammed, we simply use the identity matrix to define the first 3 residues\n",
    "        self.A = torch.tensor([0., 0., 1.])\n",
    "        self.B = torch.tensor([0., 1., 0.])\n",
    "        self.C = torch.tensor([1., 0., 0.])\n",
    "\n",
    "        #bond length vectors C-N, N-CA, CA-C\n",
    "        self.avg_bond_lens = torch.tensor([1.329, 1.459, 1.525])\n",
    "        #bond angle vector, in radians, CA-C-N, C-N-CA, N-CA-C\n",
    "        self.avg_bond_angles = torch.tensor([2.034, 2.119, 1.937])\n",
    "    \n",
    "    def forward(self, prev_aa, last_hidden, enc_out, prev_pred_coords):\n",
    "        \n",
    "        #forward propagate lstm\n",
    "        emb = self.embeds(prev_aa)\n",
    "        attn_weights = self.attn(last_hidden, enc_out).unsqueeze(1)\n",
    "        context = attn_weights.bmm(enc_out.transpose(0, 1))\n",
    "        context = context.transpose(0, 1)\n",
    "        \n",
    "        rnn_input = torch.cat((emb, context), 2)\n",
    "        gru_out, hid_out = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        sin_out = self.hardtanh(self.linear1(gru_out))\n",
    "        cos_out = self.hardtanh(self.linear2(gru_out))\n",
    "        out = torch.atan2(sin_out, cos_out).squeeze(0)\n",
    "\n",
    "        new_pred_coords = geometric_unit(prev_pred_coords, out, \n",
    "                                         self.avg_bond_angles, self.avg_bond_lens)\n",
    "\n",
    "        return new_pred_coords, hid_out, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "\n",
    "encoder = ProteinEncoder(aa2vec, hidden_size, num_layers)\n",
    "decoder = ProteinDecoder(aa2vec, hidden_size, num_layers)\n",
    "\n",
    "enc_opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "dec_opt = torch.optim.Adam(decoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 11.0974680583\n",
      "Epoch 1, Loss 10.6576738358\n",
      "Epoch 2, Loss 10.8651529948\n",
      "Epoch 3, Loss 10.022840182\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([0., 0., 1.])\n",
    "B = torch.tensor([0., 1., 0.])\n",
    "C = torch.tensor([1., 0., 0.])\n",
    "\n",
    "drmsd = dRMSD()\n",
    "running_loss=0.0\n",
    "\n",
    "for epoch in range(50):\n",
    "    pa=3\n",
    "    for i, data in enumerate(trn_data):\n",
    "        try:\n",
    "            inp_seq = torch.tensor(data['sequence'], dtype=torch.long, requires_grad=False)\n",
    "            inp_lens = torch.tensor(data['length'], dtype=torch.long, requires_grad=False)\n",
    "            gt_coords = data['coords']\n",
    "\n",
    "            enc_out, enc_hi = encoder([inp_seq, inp_lens])\n",
    "\n",
    "            # Prepare decoder input and outputs\n",
    "            prev_aa = inp_seq[0].unsqueeze(0)\n",
    "            last_hidden = enc_hi[:decoder.num_layers]\n",
    "\n",
    "            broadcast = torch.ones((inp_seq.size(1), 3))\n",
    "            new_pred_coords = torch.stack([A*broadcast, B*broadcast, C*broadcast])\n",
    "\n",
    "            # Run through decoder one time step at a time\n",
    "            for t in range(1, max(inp_lens)):\n",
    "                new_pred_coords, last_hidden, decoder_attn = decoder(\n",
    "                    prev_aa, last_hidden, enc_out, new_pred_coords\n",
    "                )\n",
    "                prev_aa = inp_seq[t].unsqueeze(0) # Next input is current target\n",
    "\n",
    "            enc_opt.zero_grad()\n",
    "            dec_opt.zero_grad()\n",
    "\n",
    "            loss = drmsd(new_pred_coords, gt_coords)\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1)\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=1)\n",
    "\n",
    "            enc_opt.step()\n",
    "            dec_opt.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i != 0) and (i % 2 == 0):\n",
    "                print('Epoch {}, Loss {}'.format(epoch, running_loss/pa))\n",
    "                running_loss = 0.0\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "            \n",
    "        except:\n",
    "            pa-=1\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
